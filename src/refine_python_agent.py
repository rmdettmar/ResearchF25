import json
from pathlib import Path
from typing import Dict, List, Tuple
import argparse
import os
from datetime import datetime
import ast
import json
from typing import Dict
import python_call as py
from llama_index.core.base.llms.types import ChatMessage, MessageRole
from .check_consistency import ConsistencyChecker
from .check_consistency_cmb import ConsistencyChecker_cmb
from .mage.gen_config import get_llm
from .mage.log_utils import get_logger
from .mage.prompts import ORDER_PROMPT
from .mage.token_counter import TokenCounter, TokenCounterCached
from .mage.gen_config import Config
from .mage.log_utils import get_logger, set_log_dir, switch_log_to_file
logger = get_logger(__name__)

SYSTEM_PROMPT = r"""
You are an expert in RTL design and verification.
Your task is to review a natural-language RTL specification, a python code that realize the function described in the specification, and a report from another judge agent if there are mismatches of the testbench generated by the python code.
You must judge whether the report is really correct and think about how to refine the python code according to the report to realize the real function described in the specification. Please note that you can only refine the python function node, not the function interface and the return value.

"""

INIT_EDITION_PROMPT = """
First, carefully read the following RTL specification:

<specification>
{specification}
</specification>

Now, examine the Python code that attempts to implement this specification:

<python_code>
{python_code}
</python_code>

Next, review the report from another judge agent that highlights mismatches between the testbench generated by the Python code and the specification:

<judge_report>
{judge_report}
</judge_report>

Your task is to:

1. Think why the python code is not correct.
2. Provide a refined version of the Python code that addresses any identified issues.

Importance]
### 1. Data structure transfer
In RTL descriptions, a signal is typically defined with a range notation like [m:n]:

The first number (m) is the leftmost position in the bit vector
The second number (n) is the rightmost position
String to Bit Position Mapping
Examine each input combination and its corresponding output position:
For descending order [m] where m > n (typical RTL):

If a signal is defined as x[4:0], then the binary value '11100' corresponds to:

x4=1 (leftmost digit in string)
x3=1
x2=1
x1=0
x0=0 (rightmost digit in string)


If a signal is defined as x[3:1], then the binary value '100' corresponds to:

x3=1 (leftmost digit in string)
x2=0
x1=0 (rightmost digit in string)

To automate this process, you must generate a Python dictionary that decodes a binary string into the corresponding RTL signal assignments by associating each bit with its correct index based on the signal's declared range.
like:
1. in_dict = {{f"in\[{{msb - i}}\]": int(b) for i, b in enumerate(in)}}

# Example usage: for a signal defined as x[3:1], the binary string '100' maps to x['3']=1, x["2"]=0, and x['1']=0.
result = binary_to_rtl_dict("x", 3, 1, "100")
print(result)  # Output: {{'3': 1, '2': 0, '1': 0}}

### 4. Error Handling and Edge Cases:
   - Implement appropriate error handling for invalid inputs or unexpected conditions.
   - Consider edge cases that might arise from the RTL specification and handle them accordingly.


Important notes:
- You only need to refine class GoldenDUT in the code: init and load. keep other parts of the code unchanged.
- You may only refine the init and load method body. Do not modify the method interface or return value.
- Ensure that your refinements are based on the original specification and not just the judge's report, as the report may contain errors.
- If you believe the judge's report is incorrect or irrelevant in any way, explain why in your analysis.
[Hint]

0. Perform bitwise consistency checks for all 01 sequences: Confirm input/output bit lengths match. Verify no duplicate minterms in truth tables. Cross-check Karnaugh map groupings against standard adjacency rules. When detecting non-standard ordering in inputs, check the order of outputs. 

1. Karnaugh Maps:
example:
// ab
// cd 00 01 11 10
// 00 | 1 | 0 | 1 | 1 |
// 01 | 0 | 1 | 0 | 1 |
// 11 | 1 | 1 | 0 | 0 |
// 10 | 1 | 0 | 0 | 0 |
To interpret the table:
The columns (left to right) represent the values of ab = 00, 01, 11, 10
The rows (top to bottom) represent the values of cd = 00, 01, 11, 10
Each cell contains the function output f(a, b, c, d) for the corresponding combination of a, b, c, and d.
Make sure that the key 'abcd' is constructed with: a and b from the column label (left to right: 00, 01, 11, 10), c and d from the row label (top to bottom: 00, 01, 11, 10), So the top-third cell corresponds to a=1, b=1, c=0, d=0 → '0011'
eg. For a = 1, b = 1, c = 1, d = 0, look at row cd = 10 and column ab = 11; the value is 0, so f(1, 1, 1, 0) = 0.
For a = 1, b = 0, c = 1, d = 0, look at row cd = 10 and column ab = 10; the value is 0, so f(1, 0, 1, 0) = 0. 

3. For finite state machine, the next state is determined by the current state and the input. You need to generate the truth table which includes all the possible combinations of the current state and the input. For example,    
 _TRUTH_TABLE = {{
            '0000': '1',  # S0 + w=0 → S1 → y0 = 1
            '0001': '0',  # S0 + w=1 → S2 → y0 = 0
            '0010': '1',  # S1 + w=0 → S3 → y0 = 1
            '0011': '0',  # S1 + w=1 → S4 → y0 = 0
            '0100': '0',  # S2 + w=0 → S4 → y0 = 0
            '0101': '1',  # S2 + w=1 → S5 → y0 = 1
            '0110': '1',  # S3 + w=0 → S5 → y0 = 1
            '0111': '0',  # S3 + w=1 → S0 → y0 = 0
            
            
        }}


When encountering Karnaugh maps in specifications:
-  Please construct a `_TRUTH_TABLE` dictionary representing the circuit logic, where:
   - Each key is a binary string representing the input combination, ordered using **Gray code** for Karnaugh map alignment.
   Make sure that the key 'abcd' is constructed with: a and b from the column label (left to right: 00, 01, 11, 10), c and d from the row label (top to bottom: 00, 01, 11, 10), So the top-third cell corresponds to a=1, b=1, c=0, d=0 → '0011'.

   - Each value is either 0 or 1, corresponding to the output for that input.
   - Don't-care (`d`) entries should be resolved in a way that simplifies logic (you may assign them to 0).
   - For any unspecified or ambiguous input (e.g., variables named `x` or unused in K-map), default the value to 0.
- Follow these rules strictly:
   - All input variables must be used in the Gray code order to construct the lookup key.
   - If a variable does not appear in the Karnaugh map (e.g., labeled `x` or not mentioned), treat it as `0` during simulation.
   - Only logic lookup is allowed, no procedural conditionals like `if/else` are permitted.


<refined_instruction>
{refined_instruction}
</refined_instruction>
"""
def restructured_data(file_path,new_file_path):
    with open(file_path, 'r') as f:
        json_string = f.read()
    json_data = json.loads(json_string)

    # Create a new list to store restructured data
    restructured_data = []

    # Iterate over the original JSON data
    for scenario_data in json_data:
        scenario_name = scenario_data["scenario"]
        input_vars = scenario_data["input variable"]
        output_vars = scenario_data["output variable"]
        
        # Iterate over each input variable group
        for i, input_var in enumerate(input_vars):
            # Build a new element
            new_element = {
                "scenario": f"{scenario_name}_{i}",
                "clock cycles": input_var["clock cycles"],
                "input variable": input_var,
                "output variable": output_vars[i] if i < len(output_vars) else None
            }
            
            restructured_data.append(new_element)

    # Output the restructured data
    with open(new_file_path, 'w') as f:
        json.dump(restructured_data, f, indent=2)


Instructions_for_Python_Code = """
Include the full definition of the GoldenDUT class, but only modify the body of its load method. Your code will be followed by:
'''python
def check_output(stimulus_list_scenario):

    dut = GoldenDUT()
    tb_outputs = []


    for stimulus_list in stimulus_list_scenario\["input variable"\]:


        clock_cycles = stimulus_list\['clock cycles']]
        clk = 1
        input_vars_list = {{k: v for k, v in stimulus_list.items() if k != "clock cycles"}}
        output_vars_list = {{'clock cycles':clock_cycles}}

        for i in range(clock_cycles):
            input_vars = {{k:v\[i]] for k,v in input_vars_list.items()}}

            output_vars = dut.load(clk,input_vars)
            for k,v in output_vars.items():
                if k not in output_vars_list:
                    output_vars_list\[k\] = []
                output_vars_list\[k\].append(v)
            


        tb_outputs.append(output_vars_list)

    return tb_outputs

if __name__ == "__main__":

    with open("stimulus.json", "r") as f:
        stimulus_data = json.load(f)


    if isinstance(stimulus_data, dict):
        stimulus_list_scenarios = stimulus_data.get("input variable", \[\])
    else:
        stimulus_list_scenarios = stimulus_data

    outputs=\[\]
    for stimulus_list_scenario in stimulus_list_scenarios:
        outputs.append( check_output(stimulus_list_scenario))

    print(json.dumps(outputs, indent=2))








'''
 2. Signal Loading and State Updates (load method)

Implement the method exactly as shown:

def load(self, clk, stimulus_dict: Dict[str, List[str]]):
    '''
    clk: the clock signal, 1 for high, 0 for low
    stimulus_dict: a dictionary formatted not include clock cycles as follows:
    {{"input_variable_name1": (a binary sequence string)input_variable_value1,
    "input_variable_name2": (a binary sequence string)input_variable_value2,
    "input_variable_name3": (a binary sequence string)input_variable_value3}}
    Parse each input variable and use it to perform RTL state updates.
    Please note input variable is in string format and you need to convert it to the corresponding type.
    Returns a dictionary of the outputs aligned with the RTL module outputs and updated states for verification. The format of the output dictionary is as follows:
    {{"output_variable_name1": (a binary sequence string)output_variable_value1,
    "output_variable_name2": (a binary sequence string)output_variable_value2,
    "output_variable_name3": (a binary sequence string)output_variable_value3}}
    '''
    pass  # Implement your signal update logic here


<refined_code>
[Your refined Python code here, including all the code of class GoldenDUT, but only refine the load method body]
</refined_code>

Be thorough in your analysis and clear in your explanations. Your goal is to produce a refined Python implementation that accurately reflects the functionality described in the original RTL specification.
"""
Head='''

import json
from typing import Dict, List, Union
'''

CMB_TAIL = """
def check_output(stimulus):

    dut = GoldenDUT()


        

    return dut.load(stimulus)

if __name__ == "__main__":

    with open("stimulus.json", "r") as f:
        stimulus_data = json.load(f)

    stimulus_list = []
    for stimulus in stimulus_data:
        stimulus_list.append(stimulus['input variable'])

    tb_outputs = []
    for stimulus in stimulus_list:
        scenario_outputs=[]
        for cycle in stimulus:

            outputs = check_output(cycle)
            scenario_outputs.append(outputs)
        tb_outputs.append(scenario_outputs)


    

    print(json.dumps(tb_outputs, indent=2))


"""


SEQ_TAIL='''


def check_output(stimulus_list_scenario):

    dut = GoldenDUT()
    tb_outputs = []


    for stimulus_list in stimulus_list_scenario["input variable"]:


        clock_cycles = stimulus_list['clock cycles']
        clk = 1
        input_vars_list = {k: v for k, v in stimulus_list.items() if k != "clock cycles"}
        output_vars_list = {'clock cycles':clock_cycles}
        for k,v in input_vars_list.items():
            if len(v) < clock_cycles:
                v.extend([v[-1]] * (clock_cycles - len(v)))
                
        

        for i in range(clock_cycles):
            input_vars = {k:v[i] for k,v in input_vars_list.items()}

            output_vars = dut.load(clk,input_vars)
            for k,v in output_vars.items():
                if k not in output_vars_list:
                    output_vars_list[k] = []
                output_vars_list[k].append(v)
            


        tb_outputs.append(output_vars_list)

    return tb_outputs

if __name__ == "__main__":
    stimulus_file_name = "stimulus.json"
    with open(stimulus_file_name, "r") as f:
        stimulus_data = json.load(f)


    if isinstance(stimulus_data, dict):
        stimulus_list_scenarios = stimulus_data.get("input variable", [])
    else:
        stimulus_list_scenarios = stimulus_data

    outputs=[]
    for stimulus_list_scenario in stimulus_list_scenarios:
        outputs.append( check_output(stimulus_list_scenario))
    with open(stimulus_file_name, "w") as f:
        json.dump(stimulus_list_scenarios, f, indent=4)

    print(json.dumps(outputs, indent=2))




'''

EXTRA_ORDER_PROMPT = r"""
VERY IMPORTANT: Please only include "reasoning" and "result" in your response.
Do not include any other information in your response, like 'json', 'example', 'Let me analyze','input_spec' or '<output_format>'.
Key instruction: Direct output, no extra comments.
As a reminder, please directly provide the content without adding any extra comments or explanations.
"""

EXAMPLE_OUTPUT_FORMAT = {
    "analysis": "All reasoning steps",
    
    "refined_code": "The refined Python code",
}

ACTION_OUTPUT_PROMPT = r"""
Output after running given action:
<action_output>
{action_output}
</action_output>
"""

EXAMPLE_OUTPUT = {
    "analysis": "All reasoning steps",
    
    "refined_code": "The refined Python code",
}



def process_testbench(json_file):
    # Read the JSON file
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    # Convert to the required dictionary format
    testbench = []
    for scenario in data:
        for i in range(len(scenario['input variable'])):
            print(f"{scenario['output variable'][i]}: {scenario['input variable'][i]}")
            scenario_name = scenario['scenario']+str(i)
            testbench.append({
                'scenario': scenario_name,
                'input variable':[ scenario['input variable'][i]],
                'output variable': [scenario['output variable'][i]]
            })
    
    return testbench

def create_testbench_json(stimulus_file, output_file, output_json_file):
   
    # Read stimulus.json to get input data
    with open(stimulus_file, "r") as f:
        stimulus_data = json.load(f)

    # Read our_output.txt to get output data
    with open(output_file, "r") as f:
        output_lines = f.readlines()

    # Parse output data
    all_outputs = []
    for line in output_lines:

        parsed = ast.literal_eval(line)
        if parsed[0] and "out" in parsed[1]:
            # Parse JSON string
            output_data = json.loads(parsed[1]["out"])
            all_outputs.extend(output_data)  # Unpack output data list

    # If no valid output, exit
    if not all_outputs:
        print("No valid output data found")
        return

    # Use the last complete output as the standard result
    standard_output = all_outputs  # Use the last output
    #print(standard_output)
    # Merge input and output
    combined_data = []

    # Iterate over each test scenario
    for i, stimulus_scenario in enumerate(stimulus_data):
        scenario_name = stimulus_scenario["scenario"]
        # print(stimulus_scenario['input variable'][i]['clock cycles'])
        #print(standard_output[i])
        # Create combined scenario data
        for j in stimulus_scenario["input variable"]:
            for key,item in j.items():
                if key!="clock cycles":
                    if len(item) < j["clock cycles"]:
                        print(f"item: {item} is less than clock cycles: {j['clock cycles']}")
                        item.extend([item[-1]]*(j["clock cycles"]-len(item)))
        combined_scenario = {
            "scenario": scenario_name,
            "input variable": stimulus_scenario["input variable"],
        }
        combined_scenario["output variable"] = []
        
        for x, j in enumerate(combined_scenario["input variable"]):
            #print("j", j)
            temp_output = {}
            temp_output["clock cycles"] = j["clock cycles"]
            #print("standard_output[i][x]", standard_output[i][x])
            temp_output.update(standard_output[i][x])
            combined_scenario["output variable"].append(temp_output)
        combined_data.append(combined_scenario)

    # Write combined data to JSON file
    with open(output_json_file, "w", encoding="utf-8") as f:
        json.dump(combined_data, f, indent=2, ensure_ascii=False)

    print(f"Successfully merged stimulus and output data to {output_json_file}")


def create_testbench_json_cmb(stimulus_file, output_file, output_json_file):
    """
    Merge stimulus.json and our_output.txt into a complete testbench.json file
    
    Parameters:
    stimulus_file -- stimulus.json file path
    output_file -- our_output.txt file path
    output_json_file -- output testbench.json file path
    """
    # Read stimulus.json to get input data
    with open(stimulus_file, 'r') as f:
        stimulus_data = json.load(f)
    
    # Read our_output.txt to get output data
    with open(output_file, 'r') as f:
        output_lines = f.readlines()
    all_outputs = []
    for line in output_lines:

        parsed = ast.literal_eval(line)
        if parsed[0] and "out" in parsed[1]:
            # Parse JSON string
            output_data = json.loads(parsed[1]["out"])
            all_outputs.extend(output_data)  # Unpack output data list
    

    # If no valid output, exit
    if not all_outputs:
        print("No valid output data found")
        return
    
    # Check for inconsistencies in all output groups
    # Only use the first group output as the standard result
    standard_output = all_outputs
    print(f"standard_output: {standard_output}")
    
    # Merge input and output
    combined_data = []
    
    # Iterate over each test scenario
    for i, stimulus_scenario in enumerate(stimulus_data):
        scenario_name = stimulus_scenario['scenario']
        
        # Find corresponding scenario in output data
        output_scenario = standard_output[i]
        
        if output_scenario:
            # Create combined scenario data
            combined_scenario = {
                "scenario": scenario_name,
                "input variable": stimulus_scenario['input variable'],
                "output variable": output_scenario
            }
            combined_data.append(combined_scenario)
        else:
            print(f"Warning: No scenario '{scenario_name}' found in output data")
            # Add scenario with no output
            combined_scenario = {
                "scenario": scenario_name,
                "input variable": stimulus_scenario['input variable'],
                "output variable": []
            }
            combined_data.append(combined_scenario)
    
    # Write merged data to JSON file
    with open(output_json_file, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, indent=2, ensure_ascii=False)
    
    print(f"Successfully merged stimulus and output data to {output_json_file}")

  

class RefinePythonAgent:
    def __init__(
        self,
        model: str,
        max_token: int,
        provider: str,
        cfg_path: str,
        top_p: float,
        temperature: float,
        exp_dir: str,
        task_numbers: int,
    ):
        self.model = model
        self.llm = get_llm(
            model=model,
            max_token=max_token,
            provider=provider,
            cfg_path=cfg_path,
            top_p=top_p,
            temperature=temperature,
        )
        self.token_counter = (
            TokenCounterCached(self.llm)
            if TokenCounterCached.is_cache_enabled(self.llm)
            else TokenCounter(self.llm)
        )
        self.exp_dir = Path(exp_dir+f"/{task_numbers}")

    def get_init_prompt_messages(self,i: int) -> List[ChatMessage]:
        """Generate initial prompt messages."""
        system_prompt = ChatMessage(content=SYSTEM_PROMPT, role=MessageRole.SYSTEM)

        spec, python_code, judge_report = self.load_input_files(i)

        init_prompt = ChatMessage(
            content=INIT_EDITION_PROMPT.format(
                specification=spec, python_code=python_code, judge_report=judge_report,refined_instruction=Instructions_for_Python_Code
            ),
            role=MessageRole.USER,
        )

        return [system_prompt, init_prompt]

    def get_order_prompt_messages(self) -> List[ChatMessage]:
        """Generate order prompt messages."""
        return [
            ChatMessage(
                    content=ORDER_PROMPT.format(
                        output_format="".join(
                            json.dumps(EXAMPLE_OUTPUT_FORMAT, indent=4)
                        )
                    ),
                    role=MessageRole.USER,
                ),
        ]


    def load_input_files(self,i: int) -> Tuple[str, str, str]:
        """Load the spec, scenario description and testbench files."""

        with open(self.exp_dir / "spec.txt", "r") as f:
            spec = f.read()
        if i==0:
            with open(self.exp_dir / "pychecker_0.py", "r") as f:
                python_code = f.read()
        else:
            with open(self.exp_dir / f"refined_code.py", "r") as f:
                python_code = f.read()

        with open(self.exp_dir / "output.txt", "r") as f:
            judge_report = f.read()

        return spec, python_code, judge_report

    def run(self,i: int,circuit_type: str) -> bool:
        """
        Main function to check consistency and fix implementation if needed.
        Returns True if all scenarios match after potential fixes.
        """
        """Single chat interaction to check consistency."""
        #spec, scenario, testbench = self.load_input_files()
        if isinstance(self.token_counter, TokenCounterCached):
            self.token_counter.set_enable_cache(True)
        self.token_counter.set_cur_tag(self.__class__.__name__)

        # Generate response
        messages = self.get_init_prompt_messages(i) + self.get_order_prompt_messages()
        logger.info(f"Consistency checker input message: {messages}")
        resp, token_cnt = self.token_counter.count_chat(messages)
        logger.info(f"Token count: {token_cnt}")
        logger.info(f"Response: {resp.message.content}")
        

        #response_content = resp.message.content
        try:
                # output_json_obj: Dict = json.loads(response.message.content, strict=False)

                # use this for Deepseek r1 and claude-3-5-sonnet
                # if self.model == "claude-3-5-sonnet-20241022":
                #     output_json_obj: Dict = json.loads("".join(response.choices[0].message.content.split("\n")[1:]), strict=False)
                # else:
                #     output_json_obj: Dict = json.loads(response.choices[0].message.content, strict=False)
                output_json_obj: Dict = json.loads(resp.message.content, strict=False)
                python_code = Head
                python_code+='\n\n'
                python_code+=output_json_obj["refined_code"]
                python_code+='\n\n'
                if circuit_type == "CMB":
                    python_code+=CMB_TAIL
                else:
                    python_code+=SEQ_TAIL
                with open(self.exp_dir / "refined_code.py", "w") as f:
                    f.write(python_code)

                
        except json.decoder.JSONDecodeError as e:
                    print(f"Json parse error: {e}")
                    logger.info(f"Json parse error: {e}")
                    print(resp)
                    return None
        

            # Run consistency check again with new implementation
            # Note: You might want to implement a mechanism to use the new file
            # return check_and_fix_implementation(exp_dir, token_counter)

        return python_code



args_dict = {
    # "model": "deepseek-reasoner",
    # "model": "gpt-4o-2024-08-06",
    # "model": "gpt-4o-mini-2024-07-18",
    # "model": "gemini-2.0-flash",
    # "model": "claude-3-5-sonnet-v2@20241022",
    # "model_fixer": "models/gemini-2.0-flash",
    # "model_fixer": "claude-3-5-sonnet-20241022",
    # "model_fixer": "gpt-4o-2024-08-06",
    # "provider": "anthropic",
    # "provider": "openai",
    # "provider_fixer": "anthropic",
    # "provider_fixer": "openai",
    "temperature": 0,
    "top_p": 1,
    "temperature_sample": 0.3,
    "top_p_sample": 0.95,
    "max_token": 8096,
    "model": "claude-3-5-sonnet-20241022",
    # "model_fixer": "gpt-4o-2024-08-06",
    # "provider": "anthropic",
    #"provider": "openai",
    "provider": "anthropic",
    # "model": "claude-3-7-sonnet@20250219",
    #"model": "claude-3-5-sonnet-v2@20241022",
    #"provider": "vertexanthropic",
    "provider_fixer": "vertexanthropic",
    # "task_numbers": [50],
    "task_numbers": [2, 22, 82,109, 121, 140],
    "circuit_type": "CMB",
    # "filter_instance": "Prob051|Prob052|Prob053|Prob054|Prob055|Prob101|Prob102|Prob103|Prob104|Prob105",
    # "filter_instance": "Prob092",
    # "filter_instance": "",
    "folder_path": "../verilog-eval/HDLBits/HDLBits_data_backup0304.jsonl",
    "run_identifier": "mismatch_report_for_correctness",
    "key_cfg_path": "../key.cfg",
    "use_golden_ref": True,
    "max_trials": 5,
    "exp_dir": "output_tb_gen_tb_20250406"
}



def main():
    # Example usage
    
    args = argparse.Namespace(**args_dict)
    Config(args.key_cfg_path)
    switch_log_to_file()
    timestamp = datetime.now().strftime("%Y%m%d")
    output_dir = f"{args.run_identifier}_{timestamp}"
    log_dir = f"log_{args.run_identifier}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    results=[]
    correct_cases=[ 55, 56]
    not_identify_mistake=[]
    wrong_identify_correct_cases=[]
    summary_txt= ""
   
    for task_number in args.task_numbers:
        if args.circuit_type == "CMB":
            consistency_checker = ConsistencyChecker_cmb(args.model, args.max_token, args.provider, args.key_cfg_path, args.top_p, args.temperature, args.exp_dir,task_number)
        else:
            consistency_checker = ConsistencyChecker(args.model, args.max_token, args.provider, args.key_cfg_path, args.top_p, args.temperature, args.exp_dir,task_number)

        set_log_dir(log_dir)
        for i in range(args.max_trials):
            if consistency_checker.run()==0:
                break
            else:
                    
            
                refine_python_agent = RefinePythonAgent(args.model, args.max_token, args.provider, args.key_cfg_path, args.top_p, args.temperature, args.exp_dir, task_number)
                refined_python_code = refine_python_agent.run(i,args.circuit_type)

                    # subproc_call(f"cd {output_dir_per_task}", timeout=120)
                    # subproc_call(f"cd {output_dir_per_task}", timeout=120)
                
                output_results=py.python_call_and_save(
                        f"{args.exp_dir}/{task_number}/refined_code.py", silent=True, timeout=120)

                # Merge results from the list into a string
                try:
                    output_str = "\n".join(str(result) for result in output_results)
                    output_file_path = os.path.join(args.exp_dir, f"{task_number}/refined_output.txt")
                    with open(output_file_path, "w") as output_file:
                        output_file.write(output_str)
                    if args.circuit_type == "CMB":
                        create_testbench_json_cmb(f"{args.exp_dir}/{task_number}/stimulus.json",f"{args.exp_dir}/{task_number}/refined_output.txt",f"{args.exp_dir}/{task_number}/testbench.json")
                    else:
                        create_testbench_json(f"{args.exp_dir}/{task_number}/stimulus.json",f"{args.exp_dir}/{task_number}/refined_output.txt",f"{args.exp_dir}/{task_number}/testbench.json")
                except Exception as e:
                    logger.error(f"Error writing output file: {e}")
                    logger.error(f"Output results: {output_results}")
             
    


    

if __name__ == "__main__":
    main()